{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124"
      ],
      "metadata": {
        "id": "VKZmVq593NWr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "id": "-4wKhCObbhO8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy==1.26"
      ],
      "metadata": {
        "id": "WDBxrtM2Qth7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#HinGe model - Best performing model (final model)\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments,AutoModelForMaskedLM\n",
        "from datasets import Dataset\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "\n",
        "# Load the dataset\n",
        "racial_train = pd.read_csv('Racial_train.csv')\n",
        "racial_val = pd.read_csv('Racial_val.csv')\n",
        "thar_dataset = pd.read_csv('THAR-Dataset.csv')\n",
        "\n",
        "def balance_dataset(df, label_column):\n",
        "    df[label_column] = df[label_column].astype(int)  # Ensure labels are integers\n",
        "    majority_class = df[df[label_column] == 0]\n",
        "    minority_class = df[df[label_column] == 1]\n",
        "    minority_class_upsampled = minority_class.sample(len(majority_class), replace=True)\n",
        "    balanced_df = pd.concat([majority_class, minority_class_upsampled])\n",
        "    return balanced_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "# Balance the dataset\n",
        "balanced_df = balance_dataset(racial_train, 'labels')\n",
        "\n",
        "# Convert labels to integers\n",
        "racial_val['labels'] = racial_val['labels'].astype(int)\n",
        "thar_dataset['labels'] = thar_dataset['labels'].replace({\"AntiReligion\":1,\"Non-AntiReligion\":0}).astype(int)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"l3cube-pune/hing-bert\")\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"l3cube-pune/hing-bert\", num_labels=2)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples['clean_text'], padding='max_length', truncation=True, max_length=128)\n",
        "\n",
        "# Convert DataFrames to Hugging Face Dataset\n",
        "train_dataset = Dataset.from_pandas(balanced_df)\n",
        "val_dataset = Dataset.from_pandas(racial_val)\n",
        "\n",
        "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
        "val_dataset = val_dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "# Ensure correct label format\n",
        "train_dataset = train_dataset.map(lambda examples: {'labels': [int(label) for label in examples['labels']]}, batched=True)\n",
        "val_dataset = val_dataset.map(lambda examples: {'labels': [int(label) for label in examples['labels']]}, batched=True)\n",
        "\n",
        "train_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
        "val_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
        "\n",
        "# Enhanced metrics function\n",
        "def compute_metrics(pred):\n",
        "    labels = pred.label_ids\n",
        "    preds = pred.predictions.argmax(-1)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n",
        "    acc = accuracy_score(labels, preds)\n",
        "    return {\n",
        "        'accuracy': acc,\n",
        "        'f1': f1,\n",
        "        'precision': precision,\n",
        "        'recall': recall\n",
        "    }\n",
        "\n",
        "# Training arguments with improved settings\n",
        "training_args1 = TrainingArguments(\n",
        "    output_dir='./results_hinge_bert',\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='./logs_hinge_bert',\n",
        "    logging_steps=10,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"f1\",\n",
        "    greater_is_better=True,\n",
        ")\n",
        "training_args2 = TrainingArguments(\n",
        "    output_dir='./results_hinge_bert',\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=2e-6,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='./logs_hinge_bert',\n",
        "    logging_steps=10,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"f1\",\n",
        "    greater_is_better=True,\n",
        ")\n",
        "\n",
        "# Process THAR dataset\n",
        "thar_train, thar_val = train_test_split(thar_dataset, test_size=0.2, random_state=42)\n",
        "\n",
        "thar_train['labels'] = thar_train['labels'].replace({\"AntiReligion\":1,\"Non-AntiReligion\":0}).astype(int)\n",
        "thar_val['labels'] = thar_val['labels'].replace({\"AntiReligion\":1,\"Non-AntiReligion\":0}).astype(int)\n",
        "\n",
        "thar_train_dataset = Dataset.from_pandas(thar_train)\n",
        "thar_val_dataset = Dataset.from_pandas(thar_val)\n",
        "\n",
        "thar_train_dataset = thar_train_dataset.map(tokenize_function, batched=True)\n",
        "thar_val_dataset = thar_val_dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "thar_train_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
        "thar_val_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
        "\n",
        "# First training on THAR dataset\n",
        "print(\"Training on THAR dataset...\")\n",
        "thar_trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args1,\n",
        "    train_dataset=thar_train_dataset,\n",
        "    eval_dataset=thar_val_dataset,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "thar_trainer.train()\n",
        "\n",
        "# Save THAR training results\n",
        "thar_eval_results = thar_trainer.evaluate()\n",
        "print(f\"THAR dataset evaluation results: {thar_eval_results}\")\n",
        "model.save_pretrained(\"./hinge_bert_thar_finetuned\")\n",
        "tokenizer.save_pretrained(\"./hinge_bert_thar_finetuned\")\n",
        "\n",
        "# Second training on racial dataset\n",
        "print(\"Training on racial dataset...\")\n",
        "trainer = Trainer(\n",
        "    model=model,  # Continue with the same model\n",
        "    args=training_args2,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "# Save final model\n",
        "model.save_pretrained(\"./hinge_bert_final_finetuned\")\n",
        "tokenizer.save_pretrained(\"./hinge_bert_final_finetuned\")\n",
        "\n",
        "# Final evaluation\n",
        "racial_eval_results = trainer.evaluate()\n",
        "print(f\"Racial dataset evaluation results: {racial_eval_results}\")\n",
        "\n",
        "# Test on a few examples\n",
        "def predict_text(text):\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "    logits = outputs.logits\n",
        "    prediction = torch.argmax(logits, dim=1).item()\n",
        "\n",
        "    return \"Racial Hoaxes detected\" if prediction == 1 else \"Not a Racial Hoaxes\"\n",
        "\n",
        "# Test examples\n",
        "test_texts = [\n",
        "    \"आप सभी बहुत अच्छे लोग हैं\",  # You all are very good people\n",
        "    \"I hate people from that community\",\n",
        "    \"यह एक सामान्य वाक्य है\",  # This is a normal sentence\n",
        "    \"These people should not be allowed in our country\"\n",
        "]\n",
        "\n",
        "print(\"\\nTesting the model on example texts:\")\n",
        "for text in test_texts:\n",
        "    print(f\"Text: {text}\")\n",
        "    print(f\"Prediction: {predict_text(text)}\\n\")"
      ],
      "metadata": {
        "id": "Piler7sc0la9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "val = pd.read_csv(\"/content/Racial_val.csv\")\n",
        "val = Dataset.from_pandas(val)\n",
        "val = val.map(tokenize_function, batched=True)\n",
        "val_pred = trainer.predict(val)\n",
        "val_preds = np.argmax(val_pred.predictions, axis=-1)\n",
        "df_preds = pd.DataFrame({'Predicted_Labels': val_preds})\n",
        "df_preds.to_csv('predictions_val.csv', index=False)\n",
        "true_labels = df_preds['Predicted_Labels']\n",
        "report = classification_report(true_labels, val['labels'])\n",
        "print(report)"
      ],
      "metadata": {
        "id": "ZbhdH25VIaQa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#cjvt/roberta-en-hi-codemixed model\n",
        "import pandas as pd\n",
        "import torch\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
        "from datasets import Dataset\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "\n",
        "# Load the dataset\n",
        "racial_train = pd.read_csv('Racial_train.csv')\n",
        "racial_val = pd.read_csv('Racial_val.csv')\n",
        "thar_dataset = pd.read_csv('THAR-Dataset.csv')\n",
        "\n",
        "def balance_dataset(df, label_column):\n",
        "    df[label_column] = df[label_column].astype(int)  # Ensure labels are integers\n",
        "    majority_class = df[df[label_column] == 0]\n",
        "    minority_class = df[df[label_column] == 1]\n",
        "    minority_class_upsampled = minority_class.sample(len(majority_class), replace=True)\n",
        "    balanced_df = pd.concat([majority_class, minority_class_upsampled])\n",
        "    return balanced_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "# Balance the dataset\n",
        "balanced_df = balance_dataset(racial_train, 'labels')\n",
        "\n",
        "# Convert labels to integers\n",
        "racial_val['labels'] = racial_val['labels'].astype(int)\n",
        "thar_dataset['labels'] = thar_dataset['labels'].replace({\"AntiReligion\":1,\"Non-AntiReligion\":0}).astype(int)\n",
        "\n",
        "# Load model and tokenizer for code-mixed English-Hindi RoBERTa\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"cjvt/roberta-en-hi-codemixed\")\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"cjvt/roberta-en-hi-codemixed\", num_labels=2)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples['clean_text'], padding='max_length', truncation=True, max_length=128)\n",
        "\n",
        "# Convert DataFrames to Hugging Face Dataset\n",
        "train_dataset = Dataset.from_pandas(balanced_df)\n",
        "val_dataset = Dataset.from_pandas(racial_val)\n",
        "\n",
        "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
        "val_dataset = val_dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "# Ensure correct label format\n",
        "train_dataset = train_dataset.map(lambda examples: {'labels': [int(label) for label in examples['labels']]}, batched=True)\n",
        "val_dataset = val_dataset.map(lambda examples: {'labels': [int(label) for label in examples['labels']]}, batched=True)\n",
        "\n",
        "train_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
        "val_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
        "\n",
        "# Enhanced metrics function\n",
        "def compute_metrics(pred):\n",
        "    labels = pred.label_ids\n",
        "    preds = pred.predictions.argmax(-1)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n",
        "    acc = accuracy_score(labels, preds)\n",
        "    return {\n",
        "        'accuracy': acc,\n",
        "        'f1': f1,\n",
        "        'precision': precision,\n",
        "        'recall': recall\n",
        "    }\n",
        "\n",
        "# Training arguments with improved settings\n",
        "training_args1 = TrainingArguments(\n",
        "    output_dir='./results_roberta_codemixed',\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='./logs_roberta_codemixed',\n",
        "    logging_steps=10,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"f1\",\n",
        "    greater_is_better=True,\n",
        ")\n",
        "training_args2 = TrainingArguments(\n",
        "    output_dir='./results_roberta_codemixed',\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=2e-6,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='./logs_roberta_codemixed',\n",
        "    logging_steps=10,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"f1\",\n",
        "    greater_is_better=True,\n",
        ")\n",
        "\n",
        "# Process THAR dataset\n",
        "thar_train, thar_val = train_test_split(thar_dataset, test_size=0.2, random_state=42)\n",
        "\n",
        "thar_train_dataset = Dataset.from_pandas(thar_train)\n",
        "thar_val_dataset = Dataset.from_pandas(thar_val)\n",
        "\n",
        "thar_train_dataset = thar_train_dataset.map(tokenize_function, batched=True)\n",
        "thar_val_dataset = thar_val_dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "thar_train_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
        "thar_val_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
        "\n",
        "# First training on THAR dataset\n",
        "print(\"Training on THAR dataset...\")\n",
        "thar_trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args1,\n",
        "    train_dataset=thar_train_dataset,\n",
        "    eval_dataset=thar_val_dataset,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "thar_trainer.train()\n",
        "\n",
        "# Save THAR training results\n",
        "thar_eval_results = thar_trainer.evaluate()\n",
        "print(f\"THAR dataset evaluation results: {thar_eval_results}\")\n",
        "model.save_pretrained(\"./roberta_codemixed_thar_finetuned\")\n",
        "tokenizer.save_pretrained(\"./roberta_codemixed_thar_finetuned\")\n",
        "\n",
        "# Second training on racial dataset\n",
        "print(\"Training on racial dataset...\")\n",
        "trainer = Trainer(\n",
        "    model=model,  # Continue with the same model\n",
        "    args=training_args2,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "# Save final model\n",
        "model.save_pretrained(\"./roberta_codemixed_final_finetuned\")\n",
        "tokenizer.save_pretrained(\"./roberta_codemixed_final_finetuned\")\n",
        "\n",
        "# Final evaluation\n",
        "racial_eval_results = trainer.evaluate()\n",
        "print(f\"Racial dataset evaluation results: {racial_eval_results}\")\n",
        "\n",
        "# Test on a few examples\n",
        "def predict_text(text):\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "    logits = outputs.logits\n",
        "    prediction = torch.argmax(logits, dim=1).item()\n",
        "\n",
        "    return \"Racial Hoaxes detected\" if prediction == 1 else \"Not a Racial Hoaxes\"\n",
        "\n",
        "# Test examples\n",
        "test_texts = [\n",
        "    \"आप सभी बहुत अच्छे लोग हैं\",  # You all are very good people\n",
        "    \"I hate people from that community\",\n",
        "    \"यह एक सामान्य वाक्य है\",  # This is a normal sentence\n",
        "    \"These people should not be allowed in our country\",\n",
        "    \"This party is amazing yaar, मैं तुम्हें later call करूंगा\"  # Code-mixed example\n",
        "]\n",
        "\n",
        "print(\"\\nTesting the model on example texts:\")\n",
        "for text in test_texts:\n",
        "    print(f\"Text: {text}\")\n",
        "    print(f\"Prediction: {predict_text(text)}\\n\")"
      ],
      "metadata": {
        "id": "qhwdpdewJE0v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "val = pd.read_csv(\"/content/Racial_val.csv\")\n",
        "val = Dataset.from_pandas(val)\n",
        "val = val.map(tokenize_function, batched=True)\n",
        "val_pred = trainer.predict(val)\n",
        "val_preds = np.argmax(val_pred.predictions, axis=-1)\n",
        "df_preds = pd.DataFrame({'Predicted_Labels': val_preds})\n",
        "df_preds.to_csv('predictions_val.csv', index=False)\n",
        "true_labels = df_preds['Predicted_Labels']\n",
        "report = classification_report(true_labels, val['labels'])\n",
        "print(report)"
      ],
      "metadata": {
        "id": "2zqQZlXmTU4z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#indic bert\n",
        "import pandas as pd\n",
        "import torch\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
        "from datasets import Dataset\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "\n",
        "# Load the dataset\n",
        "racial_train = pd.read_csv('Racial_train.csv')\n",
        "racial_val = pd.read_csv('Racial_val.csv')\n",
        "thar_dataset = pd.read_csv('THAR-Dataset.csv')\n",
        "\n",
        "def balance_dataset(df, label_column):\n",
        "    df[label_column] = df[label_column].astype(int)  # Ensure labels are integers\n",
        "    majority_class = df[df[label_column] == 0]\n",
        "    minority_class = df[df[label_column] == 1]\n",
        "    minority_class_upsampled = minority_class.sample(len(majority_class), replace=True)\n",
        "    balanced_df = pd.concat([majority_class, minority_class_upsampled])\n",
        "    return balanced_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "# Balance the dataset\n",
        "balanced_df = balance_dataset(racial_train, 'labels')\n",
        "\n",
        "# Convert labels to integers\n",
        "racial_val['labels'] = racial_val['labels'].astype(int)\n",
        "thar_dataset['labels'] = thar_dataset['labels'].replace({\"AntiReligion\":1,\"Non-AntiReligion\":0}).astype(int)\n",
        "\n",
        "# Load tokenizer and model for Indic-BERT instead of HinGE-BERT\n",
        "model_name = \"ai4bharat/indic-bert\"  # Using Indic-BERT model which supports Hindi-English\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples['clean_text'], padding='max_length', truncation=True, max_length=128)\n",
        "\n",
        "# Convert DataFrames to Hugging Face Dataset\n",
        "train_dataset = Dataset.from_pandas(balanced_df)\n",
        "val_dataset = Dataset.from_pandas(racial_val)\n",
        "\n",
        "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
        "val_dataset = val_dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "# Ensure correct label format\n",
        "train_dataset = train_dataset.map(lambda examples: {'labels': [int(label) for label in examples['labels']]}, batched=True)\n",
        "val_dataset = val_dataset.map(lambda examples: {'labels': [int(label) for label in examples['labels']]}, batched=True)\n",
        "\n",
        "train_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
        "val_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
        "\n",
        "# Enhanced metrics function\n",
        "def compute_metrics(pred):\n",
        "    labels = pred.label_ids\n",
        "    preds = pred.predictions.argmax(-1)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n",
        "    acc = accuracy_score(labels, preds)\n",
        "    return {\n",
        "        'accuracy': acc,\n",
        "        'f1': f1,\n",
        "        'precision': precision,\n",
        "        'recall': recall\n",
        "    }\n",
        "\n",
        "# Training arguments with improved settings\n",
        "training_args1 = TrainingArguments(\n",
        "    output_dir='./results_indic_bert',\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='./logs_indic_bert',\n",
        "    logging_steps=10,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"f1\",\n",
        "    greater_is_better=True,\n",
        ")\n",
        "training_args2 = TrainingArguments(\n",
        "    output_dir='./results_indic_bert',\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=2e-6,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='./logs_indic_bert',\n",
        "    logging_steps=10,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"f1\",\n",
        "    greater_is_better=True,\n",
        ")\n",
        "\n",
        "# Process THAR dataset\n",
        "thar_train, thar_val = train_test_split(thar_dataset, test_size=0.2, random_state=42)\n",
        "\n",
        "thar_train_dataset = Dataset.from_pandas(thar_train)\n",
        "thar_val_dataset = Dataset.from_pandas(thar_val)\n",
        "\n",
        "thar_train_dataset = thar_train_dataset.map(tokenize_function, batched=True)\n",
        "thar_val_dataset = thar_val_dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "\n",
        "\n",
        "thar_train_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
        "thar_val_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
        "\n",
        "# First training on THAR dataset\n",
        "print(\"Training on THAR dataset...\")\n",
        "thar_trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args1,\n",
        "    train_dataset=thar_train_dataset,\n",
        "    eval_dataset=thar_val_dataset,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "thar_trainer.train()\n",
        "\n",
        "# Save THAR training results\n",
        "thar_eval_results = thar_trainer.evaluate()\n",
        "print(f\"THAR dataset evaluation results: {thar_eval_results}\")\n",
        "model.save_pretrained(\"./indic_bert_thar_finetuned\")\n",
        "tokenizer.save_pretrained(\"./indic_bert_thar_finetuned\")\n",
        "\n",
        "# Second training on racial dataset\n",
        "print(\"Training on racial dataset...\")\n",
        "trainer = Trainer(\n",
        "    model=model,  # Continue with the same model\n",
        "    args=training_args2,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "# Save final model\n",
        "model.save_pretrained(\"./indic_bert_final_finetuned\")\n",
        "tokenizer.save_pretrained(\"./indic_bert_final_finetuned\")\n",
        "\n",
        "# Final evaluation\n",
        "racial_eval_results = trainer.evaluate()\n",
        "print(f\"Racial dataset evaluation results: {racial_eval_results}\")\n",
        "\n",
        "# Test on a few examples\n",
        "def predict_text(text):\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "    logits = outputs.logits\n",
        "    prediction = torch.argmax(logits, dim=1).item()\n",
        "\n",
        "    return \"Racial Hoaxes detected\" if prediction == 1 else \"Not a Racial Hoaxes\"\n",
        "\n",
        "# Test examples\n",
        "test_texts = [\n",
        "    \"आप सभी बहुत अच्छे लोग हैं\",  # You all are very good people\n",
        "    \"I hate people from that community\",\n",
        "    \"यह एक सामान्य वाक्य है\",  # This is a normal sentence\n",
        "    \"These people should not be allowed in our country\"\n",
        "]\n",
        "\n",
        "print(\"\\nTesting the model on example texts:\")\n",
        "for text in test_texts:\n",
        "    print(f\"Text: {text}\")\n",
        "    print(f\"Prediction: {predict_text(text)}\\n\")"
      ],
      "metadata": {
        "id": "w7NbE4w32KgO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#BAAI BGE-M3 Model LoRA fine tuning\n",
        "import pandas as pd\n",
        "import torch\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
        "from datasets import Dataset\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "from peft import get_peft_model, LoraConfig, TaskType\n",
        "\n",
        "# Load the dataset\n",
        "racial_train = pd.read_csv('Racial_train.csv')\n",
        "racial_val = pd.read_csv('Racial_val.csv')\n",
        "thar_dataset = pd.read_csv('THAR-Dataset.csv')\n",
        "\n",
        "def balance_dataset(df, label_column):\n",
        "    df[label_column] = df[label_column].astype(int)  # Ensure labels are integers\n",
        "    majority_class = df[df[label_column] == 0]\n",
        "    minority_class = df[df[label_column] == 1]\n",
        "    minority_class_upsampled = minority_class.sample(len(majority_class), replace=True)\n",
        "    balanced_df = pd.concat([majority_class, minority_class_upsampled])\n",
        "    return balanced_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "# Balance the dataset\n",
        "balanced_df = balance_dataset(racial_train, 'labels')\n",
        "\n",
        "# Convert labels to integers\n",
        "racial_val['labels'] = racial_val['labels'].astype(int)\n",
        "thar_dataset['labels'] = thar_dataset['labels'].replace({\"AntiReligion\":1,\"Non-AntiReligion\":0}).astype(int)\n",
        "\n",
        "# Load tokenizer and model for BGE-M3\n",
        "model_name = \"BAAI/bge-m3\"  # Using BGE-M3 model which is a powerful multilingual model\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "base_model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
        "\n",
        "# Configure LoRA parameters\n",
        "lora_config = LoraConfig(\n",
        "    task_type=TaskType.SEQ_CLS,          # Sequence classification task\n",
        "    r=16,                                # Rank of the update matrices\n",
        "    lora_alpha=32,                       # Alpha parameter for LoRA scaling\n",
        "    lora_dropout=0.1,                    # Dropout probability for LoRA layers\n",
        "    bias=\"none\",                         # Don't train biases\n",
        "    target_modules=[\"query\", \"key\", \"value\"]  # Target attention modules\n",
        ")\n",
        "\n",
        "# Apply LoRA adapter to the model\n",
        "model = get_peft_model(base_model, lora_config)\n",
        "\n",
        "# Print model architecture to verify\n",
        "print(f\"Model architecture: {model.__class__.__name__}\")\n",
        "print(f\"Base model size: {sum(p.numel() for p in base_model.parameters()) / 1e6:.2f}M parameters\")\n",
        "print(f\"Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad) / 1e6:.2f}M parameters\")\n",
        "trainable_params_pct = 100 * sum(p.numel() for p in model.parameters() if p.requires_grad) / sum(p.numel() for p in model.parameters())\n",
        "print(f\"Percentage of trainable parameters: {trainable_params_pct:.2f}%\")\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    # BGE-M3 can handle longer sequences, but keeping reasonable length for efficiency\n",
        "    return tokenizer(examples['clean_text'], padding='max_length', truncation=True, max_length=256)\n",
        "\n",
        "# Convert DataFrames to Hugging Face Dataset\n",
        "train_dataset = Dataset.from_pandas(balanced_df)\n",
        "val_dataset = Dataset.from_pandas(racial_val)\n",
        "\n",
        "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
        "val_dataset = val_dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "# Ensure correct label format\n",
        "train_dataset = train_dataset.map(lambda examples: {'labels': [int(label) for label in examples['labels']]}, batched=True)\n",
        "val_dataset = val_dataset.map(lambda examples: {'labels': [int(label) for label in examples['labels']]}, batched=True)\n",
        "\n",
        "# Set format for PyTorch - BGE-M3 might have additional tokens\n",
        "required_columns = ['input_ids', 'attention_mask', 'labels']\n",
        "# Check if token_type_ids is in the tokenizer output\n",
        "sample_encoding = tokenizer(\"Sample text\", return_tensors=\"pt\")\n",
        "all_columns = list(sample_encoding.keys()) + ['labels']\n",
        "all_columns = [col for col in all_columns if col in train_dataset.features]\n",
        "\n",
        "train_dataset.set_format(type='torch', columns=all_columns)\n",
        "val_dataset.set_format(type='torch', columns=all_columns)\n",
        "\n",
        "# Enhanced metrics function\n",
        "def compute_metrics(pred):\n",
        "    labels = pred.label_ids\n",
        "    preds = pred.predictions.argmax(-1)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n",
        "    acc = accuracy_score(labels, preds)\n",
        "    return {\n",
        "        'accuracy': acc,\n",
        "        'f1': f1,\n",
        "        'precision': precision,\n",
        "        'recall': recall\n",
        "    }\n",
        "\n",
        "# Training arguments with improved settings for LoRA\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results_bge_m3_lora',\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=1e-4,                # Higher learning rate can be used with LoRA\n",
        "    per_device_train_batch_size=16,    # Can use larger batch size since we're training fewer parameters\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=5,                # Can train for more epochs with LoRA\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='./logs_bge_m3_lora',\n",
        "    logging_steps=10,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"f1\",\n",
        "    greater_is_better=True,\n",
        "    fp16=True,                         # Enable mixed precision training if available\n",
        "    gradient_accumulation_steps=2,     # Can use smaller accumulation with fewer parameters\n",
        ")\n",
        "\n",
        "training_argss = TrainingArguments(\n",
        "    output_dir='./results_bge_m3_loraa',\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=1e-5,                # Higher learning rate can be used with LoRA\n",
        "    per_device_train_batch_size=16,    # Can use larger batch size since we're training fewer parameters\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=5,                # Can train for more epochs with LoRA\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='./logs_bge_m3_lora',\n",
        "    logging_steps=10,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"f1\",\n",
        "    greater_is_better=True,\n",
        "    fp16=True,                         # Enable mixed precision training if available\n",
        "    gradient_accumulation_steps=2,     # Can use smaller accumulation with fewer parameters\n",
        ")\n",
        "\n",
        "# Process THAR dataset\n",
        "thar_train, thar_val = train_test_split(thar_dataset, test_size=0.2, random_state=42)\n",
        "\n",
        "thar_train_dataset = Dataset.from_pandas(thar_train)\n",
        "thar_val_dataset = Dataset.from_pandas(thar_val)\n",
        "\n",
        "thar_train_dataset = thar_train_dataset.map(tokenize_function, batched=True)\n",
        "thar_val_dataset = thar_val_dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "thar_train_dataset = thar_train_dataset.map(lambda examples: {'labels': [int(label) for label in examples['labels']]}, batched=True)\n",
        "thar_val_dataset = thar_val_dataset.map(lambda examples: {'labels': [int(label) for label in examples['labels']]}, batched=True)\n",
        "\n",
        "thar_train_dataset.set_format(type='torch', columns=all_columns)\n",
        "thar_val_dataset.set_format(type='torch', columns=all_columns)\n",
        "\n",
        "# First training on THAR dataset\n",
        "print(\"Training on THAR dataset using LoRA...\")\n",
        "thar_trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=thar_train_dataset,\n",
        "    eval_dataset=thar_val_dataset,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "# Training with error handling\n",
        "try:\n",
        "    thar_trainer.train()\n",
        "\n",
        "    # Save THAR training results\n",
        "    thar_eval_results = thar_trainer.evaluate()\n",
        "    print(f\"THAR dataset evaluation results: {thar_eval_results}\")\n",
        "    model.save_pretrained(\"./bge_m3_thar_lora_finetuned\")\n",
        "    tokenizer.save_pretrained(\"./bge_m3_thar_lora_finetuned\")\n",
        "\n",
        "    # Second training on racial dataset\n",
        "    print(\"Training on racial dataset using LoRA...\")\n",
        "    trainer = Trainer(\n",
        "        model=model,  # Continue with the same model\n",
        "        args=training_argss,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=val_dataset,\n",
        "        compute_metrics=compute_metrics,\n",
        "    )\n",
        "\n",
        "    trainer.train()\n",
        "\n",
        "    # Save final model\n",
        "    model.save_pretrained(\"./bge_m3_final_lora_finetuned\")\n",
        "    tokenizer.save_pretrained(\"./bge_m3_final_lora_finetuned\")\n",
        "\n",
        "    # Final evaluation\n",
        "    racial_eval_results = trainer.evaluate()\n",
        "    print(f\"Racial dataset evaluation results: {racial_eval_results}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Training error: {e}\")\n",
        "    print(\"\\nFalling back to a smaller model version...\")\n",
        "\n",
        "    # Fallback to a smaller version if the main one fails\n",
        "    model_name = \"BAAI/bge-small-en-v1.5\"\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    base_model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
        "\n",
        "    # Adjust LoRA config for smaller model (might have different architecture)\n",
        "    lora_config = LoraConfig(\n",
        "        task_type=TaskType.SEQ_CLS,\n",
        "        r=8,                          # Smaller rank for smaller model\n",
        "        lora_alpha=16,\n",
        "        lora_dropout=0.1,\n",
        "        bias=\"none\",\n",
        "        target_modules=[\"query\", \"key\", \"value\", \"output.dense\"]  # May need adjustment for different model\n",
        "    )\n",
        "\n",
        "    model = get_peft_model(base_model, lora_config)\n",
        "    model.to(device)\n",
        "\n",
        "    # Update training arguments for smaller model\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir='./results_bge_small_lora',\n",
        "        eval_strategy=\"epoch\",\n",
        "        save_strategy=\"epoch\",\n",
        "        learning_rate=2e-4,\n",
        "        per_device_train_batch_size=32,\n",
        "        per_device_eval_batch_size=32,\n",
        "        num_train_epochs=5,\n",
        "        weight_decay=0.01,\n",
        "        logging_dir='./logs_bge_small_lora',\n",
        "        logging_steps=10,\n",
        "        load_best_model_at_end=True,\n",
        "        metric_for_best_model=\"f1\",\n",
        "        greater_is_better=True,\n",
        "    )\n",
        "\n",
        "    # Re-tokenize data\n",
        "    def retokenize_function(examples):\n",
        "        return tokenizer(examples['clean_text'], padding='max_length', truncation=True, max_length=128)\n",
        "\n",
        "    # Reprocess datasets with new tokenizer\n",
        "    train_dataset = Dataset.from_pandas(balanced_df)\n",
        "    val_dataset = Dataset.from_pandas(racial_val)\n",
        "    thar_train_dataset = Dataset.from_pandas(thar_train)\n",
        "    thar_val_dataset = Dataset.from_pandas(thar_val)\n",
        "\n",
        "    train_dataset = train_dataset.map(retokenize_function, batched=True)\n",
        "    val_dataset = val_dataset.map(retokenize_function, batched=True)\n",
        "    thar_train_dataset = thar_train_dataset.map(retokenize_function, batched=True)\n",
        "    thar_val_dataset = thar_val_dataset.map(retokenize_function, batched=True)\n",
        "\n",
        "    train_dataset = train_dataset.map(lambda examples: {'labels': [int(label) for label in examples['labels']]}, batched=True)\n",
        "    val_dataset = val_dataset.map(lambda examples: {'labels': [int(label) for label in examples['labels']]}, batched=True)\n",
        "    thar_train_dataset = thar_train_dataset.map(lambda examples: {'labels': [int(label) for label in examples['labels']]}, batched=True)\n",
        "    thar_val_dataset = thar_val_dataset.map(lambda examples: {'labels': [int(label) for label in examples['labels']]}, batched=True)\n",
        "\n",
        "    # Set format for PyTorch with new tokenizer\n",
        "    sample_encoding = tokenizer(\"Sample text\", return_tensors=\"pt\")\n",
        "    all_columns = list(sample_encoding.keys()) + ['labels']\n",
        "    all_columns = [col for col in all_columns if col in train_dataset.features]\n",
        "\n",
        "    train_dataset.set_format(type='torch', columns=all_columns)\n",
        "    val_dataset.set_format(type='torch', columns=all_columns)\n",
        "    thar_train_dataset.set_format(type='torch', columns=all_columns)\n",
        "    thar_val_dataset.set_format(type='torch', columns=all_columns)\n",
        "\n",
        "    # Retry training with smaller model\n",
        "    thar_trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=thar_train_dataset,\n",
        "        eval_dataset=thar_val_dataset,\n",
        "        compute_metrics=compute_metrics,\n",
        "    )\n",
        "\n",
        "    thar_trainer.train()\n",
        "\n",
        "    # Save results\n",
        "    thar_eval_results = thar_trainer.evaluate()\n",
        "    print(f\"THAR dataset evaluation results (fallback model): {thar_eval_results}\")\n",
        "    model.save_pretrained(\"./bge_small_thar_lora_finetuned\")\n",
        "    tokenizer.save_pretrained(\"./bge_small_thar_lora_finetuned\")\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=val_dataset,\n",
        "        compute_metrics=compute_metrics,\n",
        "    )\n",
        "\n",
        "    trainer.train()\n",
        "\n",
        "    racial_eval_results = trainer.evaluate()\n",
        "    print(f\"Racial dataset evaluation results (fallback model): {racial_eval_results}\")\n",
        "    model.save_pretrained(\"./bge_small_final_lora_finetuned\")\n",
        "    tokenizer.save_pretrained(\"./bge_small_final_lora_finetuned\")\n",
        "\n",
        "# Merge LoRA weights with the base model for inference\n",
        "from peft import PeftModel\n",
        "\n",
        "# Load base model and LoRA weights for inference\n",
        "def get_merged_model():\n",
        "    base_model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
        "    model_path = \"./bge_m3_final_lora_finetuned\"  # Or use fallback path if needed\n",
        "    lora_model = PeftModel.from_pretrained(base_model, model_path)\n",
        "    # Merge LoRA parameters with base model for faster inference\n",
        "    merged_model = lora_model.merge_and_unload()\n",
        "    return merged_model\n",
        "\n",
        "# For inference, use the merged model\n",
        "inference_model = model  # Use the trained model directly for testing\n",
        "inference_model.eval()\n",
        "\n",
        "# Test on a few examples\n",
        "def predict_text(text):\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=256)\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = inference_model(**inputs)\n",
        "\n",
        "    logits = outputs.logits\n",
        "    prediction = torch.argmax(logits, dim=1).item()\n",
        "\n",
        "    return \"Racial Hoaxes detected\" if prediction == 1 else \"Not a Racial Hoaxes\"\n",
        "\n",
        "# Test examples\n",
        "test_texts = [\n",
        "    \"आप सभी बहुत अच्छे लोग हैं\",  # You all are very good people\n",
        "    \"I hate people from that community\",\n",
        "    \"यह एक सामान्य वाक्य है\",  # This is a normal sentence\n",
        "    \"These people should not be allowed in our country\"\n",
        "]\n",
        "\n",
        "print(\"\\nTesting the model on example texts:\")\n",
        "for text in test_texts:\n",
        "    print(f\"Text: {text}\")\n",
        "    print(f\"Prediction: {predict_text(text)}\\n\")\n",
        "\n",
        "# Performance evaluation function\n",
        "def evaluate_model_performance(test_texts, ground_truth):\n",
        "    predictions = []\n",
        "    for text in test_texts:\n",
        "        result = predict_text(text)\n",
        "        predictions.append(1 if result == \"Racial Hoaxes\" else 0)\n",
        "\n",
        "    correct = sum(p == g for p, g in zip(predictions, ground_truth))\n",
        "    accuracy = correct / len(ground_truth)\n",
        "\n",
        "    print(f\"\\nTest accuracy: {accuracy * 100:.2f}%\")\n",
        "    print(\"Detailed results:\")\n",
        "    for i, (text, pred, truth) in enumerate(zip(test_texts, predictions, ground_truth)):\n",
        "        status = \"✓\" if pred == truth else \"✗\"\n",
        "        print(f\"{i+1}. {status} Text: \\\"{text}\\\"\")\n",
        "        print(f\"   Predicted: {'Racial Hoaxes' if pred == 1 else 'Non-Racial Hoaxes'}, Actual: {'Racial Hoaxes' if truth == 1 else 'Non-Racial Hoaxes'}\")\n",
        "\n",
        "ground_truth = [0, 1, 0, 1]\n",
        "evaluate_model_performance(test_texts, ground_truth)"
      ],
      "metadata": {
        "id": "cu3YndL-tNLN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#BAAI BGE-M3 Model full fine tuning\n",
        "import pandas as pd\n",
        "import torch\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
        "from datasets import Dataset\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "\n",
        "# Load the dataset\n",
        "racial_train = pd.read_csv('Racial_train.csv')\n",
        "racial_val = pd.read_csv('Racial_val.csv')\n",
        "thar_dataset = pd.read_csv('THAR-Dataset.csv')\n",
        "\n",
        "def balance_dataset(df, label_column):\n",
        "    df[label_column] = df[label_column].astype(int)  # Ensure labels are integers\n",
        "    majority_class = df[df[label_column] == 0]\n",
        "    minority_class = df[df[label_column] == 1]\n",
        "    minority_class_upsampled = minority_class.sample(len(majority_class), replace=True)\n",
        "    balanced_df = pd.concat([majority_class, minority_class_upsampled])\n",
        "    return balanced_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "# Balance the dataset\n",
        "balanced_df = balance_dataset(racial_train, 'labels')\n",
        "\n",
        "# Convert labels to integers\n",
        "racial_val['labels'] = racial_val['labels'].astype(int)\n",
        "thar_dataset['labels'] = thar_dataset['labels'].replace({\"AntiReligion\":1,\"Non-AntiReligion\":0}).astype(int)\n",
        "\n",
        "# Load tokenizer and model for BGE-M3\n",
        "model_name = \"BAAI/bge-m3\"  # Using BGE-M3 model which is a powerful multilingual model\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
        "\n",
        "# Print model architecture to verify\n",
        "print(f\"Model architecture: {model.__class__.__name__}\")\n",
        "print(f\"Model size: {sum(p.numel() for p in model.parameters()) / 1e6:.2f}M parameters\")\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    # BGE-M3 can handle longer sequences, but keeping reasonable length for efficiency\n",
        "    return tokenizer(examples['clean_text'], padding='max_length', truncation=True, max_length=256)\n",
        "\n",
        "# Convert DataFrames to Hugging Face Dataset\n",
        "train_dataset = Dataset.from_pandas(balanced_df)\n",
        "val_dataset = Dataset.from_pandas(racial_val)\n",
        "\n",
        "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
        "val_dataset = val_dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "# Ensure correct label format\n",
        "train_dataset = train_dataset.map(lambda examples: {'labels': [int(label) for label in examples['labels']]}, batched=True)\n",
        "val_dataset = val_dataset.map(lambda examples: {'labels': [int(label) for label in examples['labels']]}, batched=True)\n",
        "\n",
        "# Set format for PyTorch - BGE-M3 might have additional tokens\n",
        "required_columns = ['input_ids', 'attention_mask', 'labels']\n",
        "# Check if token_type_ids is in the tokenizer output\n",
        "sample_encoding = tokenizer(\"Sample text\", return_tensors=\"pt\")\n",
        "all_columns = list(sample_encoding.keys()) + ['labels']\n",
        "all_columns = [col for col in all_columns if col in train_dataset.features]\n",
        "\n",
        "train_dataset.set_format(type='torch', columns=all_columns)\n",
        "val_dataset.set_format(type='torch', columns=all_columns)\n",
        "\n",
        "# Enhanced metrics function\n",
        "def compute_metrics(pred):\n",
        "    labels = pred.label_ids\n",
        "    preds = pred.predictions.argmax(-1)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n",
        "    acc = accuracy_score(labels, preds)\n",
        "    return {\n",
        "        'accuracy': acc,\n",
        "        'f1': f1,\n",
        "        'precision': precision,\n",
        "        'recall': recall\n",
        "    }\n",
        "\n",
        "# Training arguments with improved settings\n",
        "training_args1 = TrainingArguments(\n",
        "    output_dir='./results_bge_m3',\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=1e-5,  # Lower learning rate for large models\n",
        "    per_device_train_batch_size=8,  # Smaller batch size due to model size\n",
        "    per_device_eval_batch_size=8,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='./logs_bge_m3',\n",
        "    logging_steps=10,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"f1\",\n",
        "    greater_is_better=True,\n",
        "    fp16=True,  # Enable mixed precision training if available\n",
        "    gradient_accumulation_steps=4,  # Accumulate gradients to simulate larger batch sizes\n",
        ")\n",
        "training_args2 = TrainingArguments(\n",
        "    output_dir='./results_bge_m3',\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=1e-6,  # Lower learning rate for large models\n",
        "    per_device_train_batch_size=8,  # Smaller batch size due to model size\n",
        "    per_device_eval_batch_size=8,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='./logs_bge_m3',\n",
        "    logging_steps=10,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"f1\",\n",
        "    greater_is_better=True,\n",
        "    fp16=True,  # Enable mixed precision training if available\n",
        "    gradient_accumulation_steps=4,  # Accumulate gradients to simulate larger batch sizes\n",
        ")\n",
        "\n",
        "# Process THAR dataset\n",
        "thar_train, thar_val = train_test_split(thar_dataset, test_size=0.2, random_state=42)\n",
        "\n",
        "thar_train_dataset = Dataset.from_pandas(thar_train)\n",
        "thar_val_dataset = Dataset.from_pandas(thar_val)\n",
        "\n",
        "thar_train_dataset = thar_train_dataset.map(tokenize_function, batched=True)\n",
        "thar_val_dataset = thar_val_dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "thar_train_dataset = thar_train_dataset.map(lambda examples: {'labels': [int(label) for label in examples['labels']]}, batched=True)\n",
        "thar_val_dataset = thar_val_dataset.map(lambda examples: {'labels': [int(label) for label in examples['labels']]}, batched=True)\n",
        "\n",
        "thar_train_dataset.set_format(type='torch', columns=all_columns)\n",
        "thar_val_dataset.set_format(type='torch', columns=all_columns)\n",
        "\n",
        "# First training on THAR dataset\n",
        "print(\"Training on THAR dataset...\")\n",
        "thar_trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=thar_train_dataset,\n",
        "    eval_dataset=thar_val_dataset,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "# Training with error handling\n",
        "try:\n",
        "    thar_trainer.train()\n",
        "\n",
        "    # Save THAR training results\n",
        "    thar_eval_results = thar_trainer.evaluate()\n",
        "    print(f\"THAR dataset evaluation results: {thar_eval_results}\")\n",
        "    model.save_pretrained(\"./bge_m3_thar_finetuned\")\n",
        "    tokenizer.save_pretrained(\"./bge_m3_thar_finetuned\")\n",
        "\n",
        "    # Second training on racial dataset\n",
        "    print(\"Training on racial dataset...\")\n",
        "    trainer = Trainer(\n",
        "        model=model,  # Continue with the same model\n",
        "        args=training_args1,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=val_dataset,\n",
        "        compute_metrics=compute_metrics,\n",
        "    )\n",
        "\n",
        "    trainer.train()\n",
        "\n",
        "    # Save final model\n",
        "    model.save_pretrained(\"./bge_m3_final_finetuned\")\n",
        "    tokenizer.save_pretrained(\"./bge_m3_final_finetuned\")\n",
        "\n",
        "    # Final evaluation\n",
        "    racial_eval_results = trainer.evaluate()\n",
        "    print(f\"Racial dataset evaluation results: {racial_eval_results}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Training error: {e}\")\n",
        "    print(\"\\nFalling back to a smaller model version...\")\n",
        "\n",
        "    # Fallback to a smaller version if the main one fails\n",
        "    model_name = \"BAAI/bge-small-en-v1.5\"\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
        "    model.to(device)\n",
        "\n",
        "    # Update training arguments for smaller model\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir='./results_bge_small',\n",
        "        eval_strategy=\"epoch\",\n",
        "        save_strategy=\"epoch\",\n",
        "        learning_rate=2e-5,\n",
        "        per_device_train_batch_size=16,\n",
        "        per_device_eval_batch_size=16,\n",
        "        num_train_epochs=3,\n",
        "        weight_decay=0.01,\n",
        "        logging_dir='./logs_bge_small',\n",
        "        logging_steps=10,\n",
        "        load_best_model_at_end=True,\n",
        "        metric_for_best_model=\"f1\",\n",
        "        greater_is_better=True,\n",
        "    )\n",
        "\n",
        "    # Re-tokenize data\n",
        "    def retokenize_function(examples):\n",
        "        return tokenizer(examples['clean_text'], padding='max_length', truncation=True, max_length=128)\n",
        "\n",
        "    # Reprocess datasets with new tokenizer\n",
        "    train_dataset = Dataset.from_pandas(balanced_df)\n",
        "    val_dataset = Dataset.from_pandas(racial_val)\n",
        "    thar_train_dataset = Dataset.from_pandas(thar_train)\n",
        "    thar_val_dataset = Dataset.from_pandas(thar_val)\n",
        "\n",
        "    train_dataset = train_dataset.map(retokenize_function, batched=True)\n",
        "    val_dataset = val_dataset.map(retokenize_function, batched=True)\n",
        "    thar_train_dataset = thar_train_dataset.map(retokenize_function, batched=True)\n",
        "    thar_val_dataset = thar_val_dataset.map(retokenize_function, batched=True)\n",
        "\n",
        "    train_dataset = train_dataset.map(lambda examples: {'labels': [int(label) for label in examples['labels']]}, batched=True)\n",
        "    val_dataset = val_dataset.map(lambda examples: {'labels': [int(label) for label in examples['labels']]}, batched=True)\n",
        "    thar_train_dataset = thar_train_dataset.map(lambda examples: {'labels': [int(label) for label in examples['labels']]}, batched=True)\n",
        "    thar_val_dataset = thar_val_dataset.map(lambda examples: {'labels': [int(label) for label in examples['labels']]}, batched=True)\n",
        "\n",
        "    # Set format for PyTorch with new tokenizer\n",
        "    sample_encoding = tokenizer(\"Sample text\", return_tensors=\"pt\")\n",
        "    all_columns = list(sample_encoding.keys()) + ['labels']\n",
        "    all_columns = [col for col in all_columns if col in train_dataset.features]\n",
        "\n",
        "    train_dataset.set_format(type='torch', columns=all_columns)\n",
        "    val_dataset.set_format(type='torch', columns=all_columns)\n",
        "    thar_train_dataset.set_format(type='torch', columns=all_columns)\n",
        "    thar_val_dataset.set_format(type='torch', columns=all_columns)\n",
        "\n",
        "    # Retry training with smaller model\n",
        "    thar_trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args1,\n",
        "        train_dataset=thar_train_dataset,\n",
        "        eval_dataset=thar_val_dataset,\n",
        "        compute_metrics=compute_metrics,\n",
        "    )\n",
        "\n",
        "    thar_trainer.train()\n",
        "\n",
        "    # Save results\n",
        "    thar_eval_results = thar_trainer.evaluate()\n",
        "    print(f\"THAR dataset evaluation results (fallback model): {thar_eval_results}\")\n",
        "    model.save_pretrained(\"./bge_small_thar_finetuned\")\n",
        "    tokenizer.save_pretrained(\"./bge_small_thar_finetuned\")\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args1,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=val_dataset,\n",
        "        compute_metrics=compute_metrics,\n",
        "    )\n",
        "\n",
        "    trainer.train()\n",
        "\n",
        "    racial_eval_results = trainer.evaluate()\n",
        "    print(f\"Racial dataset evaluation results (fallback model): {racial_eval_results}\")\n",
        "    model.save_pretrained(\"./bge_small_final_finetuned\")\n",
        "    tokenizer.save_pretrained(\"./bge_small_final_finetuned\")\n",
        "\n",
        "# Test on a few examples\n",
        "def predict_text(text):\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=256)\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "    logits = outputs.logits\n",
        "    prediction = torch.argmax(logits, dim=1).item()\n",
        "\n",
        "    return \"Racial Hoaxes\" if prediction == 1 else \"Non-Racial Hoaxes\"\n",
        "\n",
        "# Test examples\n",
        "test_texts = [\n",
        "    \"आप सभी बहुत अच्छे लोग हैं\",  # You all are very good people\n",
        "    \"I hate people from that community\",\n",
        "    \"यह एक सामान्य वाक्य है\",  # This is a normal sentence\n",
        "    \"These people should not be allowed in our country\"\n",
        "]\n",
        "\n",
        "print(\"\\nTesting the model on example texts:\")\n",
        "for text in test_texts:\n",
        "    print(f\"Text: {text}\")\n",
        "    print(f\"Prediction: {predict_text(text)}\\n\")\n",
        "\n",
        "# Performance evaluation function\n",
        "def evaluate_model_performance(test_texts, ground_truth):\n",
        "    predictions = []\n",
        "    for text in test_texts:\n",
        "        result = predict_text(text)\n",
        "        predictions.append(1 if result == \"Racial Hoaxes\" else 0)\n",
        "\n",
        "    correct = sum(p == g for p, g in zip(predictions, ground_truth))\n",
        "    accuracy = correct / len(ground_truth)\n",
        "\n",
        "    print(f\"\\nTest accuracy: {accuracy * 100:.2f}%\")\n",
        "    print(\"Detailed results:\")\n",
        "    for i, (text, pred, truth) in enumerate(zip(test_texts, predictions, ground_truth)):\n",
        "        status = \"✓\" if pred == truth else \"✗\"\n",
        "        print(f\"{i+1}. {status} Text: \\\"{text}\\\"\")\n",
        "        print(f\"   Predicted: {'Racial Hoaxes' if pred == 1 else 'Non-Racial Hoaxes'}, Actual: {'Racial Hoaxes' if truth == 1 else 'Non-Racial Hoaxes'}\")\n",
        "    val = pd.read_csv(\"/content/Racial_val.csv\")\n",
        "    val = Dataset.from_pandas(val)\n",
        "    val = val.map(tokenize_function, batched=True)\n",
        "    val_pred = trainer.predict(val)\n",
        "    val_preds = np.argmax(val_pred.predictions, axis=-1)\n",
        "    df_preds = pd.DataFrame({'Predicted_Labels': val_preds})\n",
        "    true_labels = df_preds['Predicted_Labels']\n",
        "    report = classification_report(true_labels, val['labels'])\n",
        "    print(report)\n",
        "\n",
        "ground_truth = [0, 1, 0, 1]\n",
        "evaluate_model_performance(test_texts, ground_truth)"
      ],
      "metadata": {
        "id": "IhVIPQQ1BBLT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_model_performance(test_texts, ground_truth)"
      ],
      "metadata": {
        "id": "CIsIj77nyG1I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "Gb_SqhiO-JrM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val = pd.read_csv(\"/content/Racial_val.csv\")\n",
        "val = Dataset.from_pandas(val)\n",
        "val = val.map(tokenize_function, batched=True)\n",
        "val_pred = trainer.predict(val)\n",
        "val_preds = np.argmax(val_pred.predictions, axis=-1)\n",
        "df_preds = pd.DataFrame({'Predicted_Labels': val_preds})\n",
        "true_labels = df_preds['Predicted_Labels']\n",
        "report = classification_report(true_labels, val['labels'])\n",
        "print(report)"
      ],
      "metadata": {
        "id": "m69sZvbt9O-M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Muril model\n",
        "import pandas as pd\n",
        "import torch\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
        "from datasets import Dataset\n",
        "\n",
        "# Load the dataset\n",
        "racial_train = pd.read_csv('Racial_train.csv')\n",
        "racial_val = pd.read_csv('Racial_val.csv')\n",
        "thar_dataset = pd.read_csv('THAR-Dataset.csv')\n",
        "\n",
        "def balance_dataset(df, label_column):\n",
        "    df[label_column] = df[label_column].astype(int)  # Ensure labels are integers\n",
        "    majority_class = df[df[label_column] == 0]\n",
        "    minority_class = df[df[label_column] == 1]\n",
        "    minority_class_upsampled = minority_class.sample(len(majority_class), replace=True)\n",
        "    balanced_df = pd.concat([majority_class, minority_class_upsampled])\n",
        "    return balanced_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "# Balance the dataset\n",
        "balanced_df = balance_dataset(racial_train, 'labels')\n",
        "\n",
        "# Convert labels to integers\n",
        "racial_val['labels'] = racial_val['labels'].astype(int)\n",
        "thar_dataset['labels'] = thar_dataset['labels'].replace({\"AntiReligion\":1,\"Non-AntiReligion\":0}).astype(int)\n",
        "\n",
        "# Load tokenizer and model\n",
        "model_name = \"google/muril-base-cased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples['clean_text'], padding='max_length', truncation=True)\n",
        "\n",
        "# Convert DataFrames to Hugging Face Dataset\n",
        "train_dataset = Dataset.from_pandas(balanced_df)\n",
        "val_dataset = Dataset.from_pandas(racial_val)\n",
        "\n",
        "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
        "val_dataset = val_dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "# Ensure correct label format\n",
        "train_dataset = train_dataset.map(lambda examples: {'labels': [int(label) for label in examples['labels']]}, batched=True)\n",
        "val_dataset = val_dataset.map(lambda examples: {'labels': [int(label) for label in examples['labels']]}, batched=True)\n",
        "\n",
        "train_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
        "val_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    eval_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=10,\n",
        ")\n",
        "\n",
        "thar_train, thar_val = train_test_split(thar_dataset, test_size=0.2, random_state=42)\n",
        "\n",
        "thar_train_dataset = Dataset.from_pandas(thar_train)\n",
        "thar_val_dataset = Dataset.from_pandas(thar_val)\n",
        "\n",
        "thar_train_dataset = thar_train_dataset.map(tokenize_function, batched=True)\n",
        "thar_val_dataset = thar_val_dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "thar_train_dataset = thar_train_dataset.map(lambda examples: {'labels': [int(label) for label in examples['labels']]}, batched=True)\n",
        "thar_val_dataset = thar_val_dataset.map(lambda examples: {'labels': [int(label) for label in examples['labels']]}, batched=True)\n",
        "\n",
        "thar_train_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
        "thar_val_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
        "\n",
        "def compute_metrics(pred):\n",
        "    logits, labels = pred\n",
        "    preds = torch.argmax(torch.tensor(logits), axis=-1)\n",
        "    return {'accuracy': (preds == torch.tensor(labels)).float().mean().item()}\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=thar_train_dataset,\n",
        "    eval_dataset=thar_val_dataset,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "eval_results = trainer.evaluate()\n",
        "print(f\"Evaluation results: {eval_results}\")"
      ],
      "metadata": {
        "id": "a6uLa5VPa2zQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check test data prediction and save it on fine tuned model"
      ],
      "metadata": {
        "id": "SwDwBRy9AhPs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"/content/Racial_test_without_labels.csv\")\n",
        "df = Dataset.from_pandas(df)\n",
        "df = df.map(tokenize_function, batched=True)\n",
        "predictions = trainer.predict(df)\n",
        "preds = np.argmax(predictions.predictions, axis=-1)\n",
        "print(preds)\n",
        "df_preds = pd.DataFrame({'Predicted_Labels': preds})\n",
        "df_preds.to_csv('predictions.csv', index=False)\n",
        "print(\"Predictions saved to predictions.csv\")\n",
        "final = pd.concat([df,df_preds],axis=1)\n",
        "final.to_csv('predictions_final.csv', index=False)"
      ],
      "metadata": {
        "id": "i-QoNmsq-_5o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report"
      ],
      "metadata": {
        "id": "vioDoKz9LxuM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check val error on fine tuned model"
      ],
      "metadata": {
        "id": "Gss1RG2tBi0O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "val = pd.read_csv(\"/content/Racial_test.csv\")\n",
        "val.drop(['ID'],axis=1,inplace=True)\n",
        "val = Dataset.from_pandas(val)\n",
        "val = val.map(tokenize_function, batched=True)\n",
        "val_pred = trainer.predict(val)\n",
        "val_preds = np.argmax(val_pred.predictions, axis=-1)\n",
        "df_preds = pd.DataFrame({'Predicted_Labels': val_preds})\n",
        "df_preds.to_csv('predictions_val.csv', index=False)\n",
        "true_labels = df_preds['Predicted_Labels']\n",
        "report = classification_report(true_labels, val['labels'])\n",
        "print(report)"
      ],
      "metadata": {
        "id": "YRx2kDHnA2wq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "xL8hLTjQ382B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#HinGe model\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments,AutoModelForMaskedLM\n",
        "from datasets import Dataset\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "\n",
        "# Load the dataset\n",
        "racial_train = pd.read_csv('Racial_train.csv')\n",
        "racial_val = pd.read_csv('Racial_val.csv')\n",
        "\n",
        "def balance_dataset(df, label_column):\n",
        "    df[label_column] = df[label_column].astype(int)  # Ensure labels are integers\n",
        "    majority_class = df[df[label_column] == 0]\n",
        "    minority_class = df[df[label_column] == 1]\n",
        "    minority_class_upsampled = minority_class.sample(len(majority_class), replace=True)\n",
        "    balanced_df = pd.concat([majority_class, minority_class_upsampled])\n",
        "    return balanced_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "# Balance the dataset\n",
        "balanced_df = balance_dataset(racial_train, 'labels')\n",
        "\n",
        "# Convert labels to integers\n",
        "racial_val['labels'] = racial_val['labels'].astype(int)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"l3cube-pune/hing-bert\")\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"l3cube-pune/hing-bert\", num_labels=2)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples['clean_text'], padding='max_length', truncation=True, max_length=128)\n",
        "\n",
        "# Convert DataFrames to Hugging Face Dataset\n",
        "train_dataset = Dataset.from_pandas(balanced_df)\n",
        "val_dataset = Dataset.from_pandas(racial_val)\n",
        "\n",
        "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
        "val_dataset = val_dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "# Ensure correct label format\n",
        "train_dataset = train_dataset.map(lambda examples: {'labels': [int(label) for label in examples['labels']]}, batched=True)\n",
        "val_dataset = val_dataset.map(lambda examples: {'labels': [int(label) for label in examples['labels']]}, batched=True)\n",
        "\n",
        "train_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
        "val_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
        "\n",
        "# Enhanced metrics function\n",
        "def compute_metrics(pred):\n",
        "    labels = pred.label_ids\n",
        "    preds = pred.predictions.argmax(-1)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n",
        "    acc = accuracy_score(labels, preds)\n",
        "    return {\n",
        "        'accuracy': acc,\n",
        "        'f1': f1,\n",
        "        'precision': precision,\n",
        "        'recall': recall\n",
        "    }\n",
        "\n",
        "# Training arguments with improved settings\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results_hinge_bert',\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='./logs_hinge_bert',\n",
        "    logging_steps=10,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"f1\",\n",
        "    greater_is_better=True,\n",
        ")\n",
        "\n",
        "\n",
        "# Second training on racial dataset\n",
        "print(\"Training on racial dataset...\")\n",
        "trainer = Trainer(\n",
        "    model=model,  # Continue with the same model\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "# Save final model\n",
        "model.save_pretrained(\"./hinge_bert_final_finetuned\")\n",
        "tokenizer.save_pretrained(\"./hinge_bert_final_finetuned\")\n",
        "\n",
        "# Final evaluation\n",
        "racial_eval_results = trainer.evaluate()\n",
        "print(f\"Racial dataset evaluation results: {racial_eval_results}\")\n",
        "\n",
        "# Test on a few examples\n",
        "def predict_text(text):\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "    logits = outputs.logits\n",
        "    prediction = torch.argmax(logits, dim=1).item()\n",
        "\n",
        "    return \"Racial Hoaxes\" if prediction == 1 else \"Non-Racial Hoaxes\"\n",
        "\n",
        "# Test examples\n",
        "test_texts = [\n",
        "    \"आप सभी बहुत अच्छे लोग हैं\",  # You all are very good people\n",
        "    \"I hate people from that community\",\n",
        "    \"यह एक सामान्य वाक्य है\",  # This is a normal sentence\n",
        "    \"These people should not be allowed in our country\"\n",
        "]\n",
        "\n",
        "print(\"\\nTesting the model on example texts:\")\n",
        "for text in test_texts:\n",
        "    print(f\"Text: {text}\")\n",
        "    print(f\"Prediction: {predict_text(text)}\\n\")"
      ],
      "metadata": {
        "id": "_wfyRp3cwnNc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}